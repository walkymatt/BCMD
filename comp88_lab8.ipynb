{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNiAQrG3q/HXqDvJg+rJmom",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/walkymatt/BCMD/blob/master/comp88_lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP0088 Lab Assignment 8\n",
        "\n"
      ],
      "metadata": {
        "id": "iZgl8O7b-fi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this final lab session, you'll generate and fit data from two latent variable models: **Gaussian Mixture Models** (GMMs) and **Hidden Markov Models** (HMMs). Both kinds of model can be fitted in the unsupervised setting using **Expectation Maximisation** approaches, in a similar fashion to the $k$-Means algorithm from last week.\n",
        "\n",
        "<!-- Examples of the kinds of plots that will be produced by your finished code are shown below. Plotting code is provided, so your plots should look pretty similar, though there is also some randomisation.\n",
        "\n",
        "![example of completed plots](https://comp0088.github.io/assets/colab/week_7.jpg) -->\n"
      ],
      "metadata": {
        "id": "N12ZHil1_ZVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up"
      ],
      "metadata": {
        "id": "hxzyJ3xeT4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, this notebook makes use of the NumPy library for numerical computing and the Matplotlib library for plotting, so we need to import them. In addition, we'll import a couple of useful statistics functions from SciPy, and also install and import an external library for Hidden Markov Models."
      ],
      "metadata": {
        "id": "4vHvSz5pReci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL8UJ7lgLznk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import multivariate_normal as mvn, norm\n",
        "\n",
        "%pip install hmmlearn\n",
        "import hmmlearn.hmm as hmm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As ever, we'll also fetch shared code from the COMP0088 GitHub:"
      ],
      "metadata": {
        "id": "K1RLN5QATflG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load lab code and resources\n",
        "!git clone https://github.com/comp0088/shared.git comp0088\n",
        "\n",
        "# at the moment this is all we care about\n",
        "import comp0088.utils as utils"
      ],
      "metadata": {
        "id": "v3X7LDC5KAob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, one last time, set up some globals for later use.\n"
      ],
      "metadata": {
        "id": "t1rjWDV14cGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the usual random number generator\n",
        "shared_rng = numpy.random.default_rng()\n",
        "\n",
        "LIMITS = (-10, 10)\n",
        "NUM_GAUSSIANS = 4\n",
        "NUM_SAMPLES = 100\n",
        "\n",
        "DARK = [plt.cm.tab20.colors[2 * ii] for ii in range(10)]\n",
        "LIGHT = [plt.cm.tab20.colors[2 * ii + 1] for ii in range(10)]"
      ],
      "metadata": {
        "id": "PfZQlfuELVwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Generate data from a Gaussian Mixture Model\n",
        "\n",
        "As in some of the previous exercises, we'll start by creating a *forward* version of the model from which we can generate sample data. We'll then attempt to fit a model to this synthetic data in the subsequent task.\n",
        "\n",
        "In a Gaussian mixture model, the data distribution is taken to be a weighted combination of several independent multivariate Gaussian distributions, each with its own **mean** vector $\\mathbf{\\mu}_i$ and **covariance** matrix $\\mathbf{\\Sigma}_i$:\n",
        "\\begin{equation}\n",
        "P(\\mathbf{x}) = \\sum_i^k \\alpha_i \\phi (\\mathbf{x} ; \\mathbf{\\mu}_i, \\mathbf{\\Sigma}_i )\n",
        "\\end{equation}\n",
        "Here $\\phi$ represents the standard multivariate Gaussian density for $\\mathbf{x} \\in \\mathbb{R}^d$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\phi(\\mathbf{x}; \\mathbf{\\mu, \\Sigma}) = \\frac{\\exp\\Big(-\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^{\\mathsf{T}}\\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu})\\Big)}{\\sqrt{(2\\pi)^d |\\mathbf{\\Sigma}|}}\n",
        "\\end{equation}\n",
        "\n",
        "The weights $\\alpha_i$ constitute a **categorical distribution**, representing the probability of a sample being drawn from each of the component Gaussians. Hence, $\\sum_i \\alpha_i = 1$ and $\\alpha_i \\geq 0 \\; \\forall i$.\n"
      ],
      "metadata": {
        "id": "FXpzvXtJAr4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Generate the data\n",
        "\n",
        "Implement `generate_gaussian_mix` in the code cell below to draw samples from the mixture of Gaussians. The parameters of all the individual Gaussians are supplied in arguments `means` and `covs`, and the categorical probabilities of drawing from each are in `class_probs`.\n",
        "\n",
        "You can use the [multinomial](https://numpy.org/devdocs/reference/random/generated/numpy.random.Generator.multinomial.html) method on `rng` to draw samples from a categorical distribution, and the [multivariate_normal](https://numpy.org/devdocs/reference/random/generated/numpy.random.Generator.multivariate_normal.html) method to draw samples from a multivariate Gaussian.\n",
        "\n",
        "Note that you are asked to return ground truth labels specifying which samples belong to which Gaussian. These labels will not be available when it comes to fitting the GMM — it's an *unsupervised learning* algorithm — but at generation time you know which distribution is responsible for each sample, and we will use that for illustrative purposes below.\n"
      ],
      "metadata": {
        "id": "RGJxos1yA34M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gaussian_mix ( num_samples, means, covs,\n",
        "                            class_probs, rng ):\n",
        "    \"\"\"\n",
        "    Draw labelled samples from a mixture of multivariate\n",
        "    gaussians.\n",
        "\n",
        "    # Arguments\n",
        "        num_samples: number of samples to generate\n",
        "            (ie, the number of rows in the returned X\n",
        "            and the length of the returned y)\n",
        "        means: a list of vectors specifying mean of each gaussian\n",
        "            (all the same length == the number of features)\n",
        "        covs: a list of covariance matrices\n",
        "            (same length as means, with each matrix being\n",
        "            num features x num features, symmetric and\n",
        "            positive semidefinite)\n",
        "        class_probs: a vector of class probabilities,\n",
        "            (same length as means, all non-negative and\n",
        "            summing to 1)\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "\n",
        "    # Returns\n",
        "        X: a matrix of sample inputs, where\n",
        "            the samples are the rows and the\n",
        "            columns are features, ie size is:\n",
        "              num_samples x num_features\n",
        "        y: a vector of num_samples labels matching\n",
        "            the samples to the gaussian from which\n",
        "            they were drawn\n",
        "    \"\"\"\n",
        "\n",
        "    # check all arguments agree on number of components\n",
        "    assert(len(means)==len(covs)==len(class_probs))\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "tbnDPv-S6pHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 1\n",
        "\n",
        "Run the cell below to generate and plot some data from your generating function.\n",
        "\n",
        "The distribution parameters are randomised each time, so you may get significantly different results when you re-run this task. Is the underlying distribution structure always visible?\n"
      ],
      "metadata": {
        "id": "NPwNjtpYh_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly generate parameters for the source distributions\n",
        "\n",
        "# means are just chosen uniformly, so may or may not be well-separated\n",
        "means = [ shared_rng.random(2) * (LIMITS[1] - LIMITS[0]) + LIMITS[0] for ii in range(NUM_GAUSSIANS) ]\n",
        "\n",
        "# covariances are also randomised, but we fudge them a bit to ensure\n",
        "# positive semi-definiteness\n",
        "covs = [ shared_rng.random((2,2)) * 2 - 1 for ii in range(NUM_GAUSSIANS) ]\n",
        "covs = [ (np.eye(2) + cc.T @ cc) for cc in covs ]\n",
        "\n",
        "# class membership is again fudged to even things out a bit\n",
        "class_probs = shared_rng.random(NUM_GAUSSIANS) + np.ones(NUM_GAUSSIANS) * 0.5\n",
        "class_probs /= np.sum(class_probs)\n",
        "\n",
        "# generate\n",
        "X, y = generate_gaussian_mix ( NUM_SAMPLES, means, covs, class_probs, shared_rng )\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "axs = fig.subplots(ncols=2)\n",
        "\n",
        "if X is None:\n",
        "    utils.plot_unimplemented(axs[0], title=f'Unlabelled data from {NUM_GAUSSIANS} Gaussians')\n",
        "    utils.plot_unimplemented(axs[1], title=f'Ground truth source distributions')\n",
        "else:\n",
        "    # one plot includes all points without source attribution\n",
        "    axs[0].scatter(X[:,0], X[:,1])\n",
        "    axs[0].set_title(f'Unlabelled data from {NUM_GAUSSIANS} Gaussians')\n",
        "    axs[0].set_xlabel('$x_1$')\n",
        "    axs[0].set_ylabel('$x_2$');\n",
        "\n",
        "    # for the other plot, we'll colour each point for its source\n",
        "    for ii in range(NUM_GAUSSIANS):\n",
        "        idx = (y == ii)\n",
        "        axs[1].scatter(X[idx,0], X[idx,1], color=LIGHT[ii])\n",
        "\n",
        "    # we're also going to add contours for each gaussian,\n",
        "    # for which we'll need a points grid\n",
        "    left, right = axs[1].get_xlim()\n",
        "    bottom, top = axs[1].get_ylim()\n",
        "    xx = np.linspace(left, right, 100)\n",
        "    yy = np.linspace(bottom, top, 100)\n",
        "    grid = np.moveaxis(np.stack(np.meshgrid(xx, yy, indexing='xy')), 0, -1)\n",
        "    extent = (left, right, bottom, top)\n",
        "\n",
        "    # draw the contours and centre points in the appropriate colours\n",
        "    for ii in range(NUM_GAUSSIANS):\n",
        "        pdf = mvn.pdf(grid, mean=means[ii], cov=covs[ii])\n",
        "        axs[1].contour(xx, yy, pdf, origin='lower', extent=extent, alpha=0.5, colors=[DARK[ii]])\n",
        "        axs[1].scatter(means[ii][0], means[ii][1], s=100, color=DARK[ii], marker='x')\n",
        "\n",
        "    axs[1].set_title(f'Ground truth source distributions')\n",
        "    axs[1].set_xlabel('$x_1$')\n",
        "    axs[1].set_ylabel('$x_2$');\n",
        "\n",
        "fig.tight_layout(pad=1)"
      ],
      "metadata": {
        "id": "m5XyX-uSFWON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Fit a Gaussian Mixture Model\n",
        "\n",
        "Gaussian mixture models are fitted by a version of the expectation-maximisation (EM) algorithm, starting from an initial guess and iteratively maximising the likelihood of the model parameters in alternating steps.\n",
        "\n",
        "In the **E-step**, soft assignments or **responsibilities**, $\\gamma_{i,j}$, are estimated for each sample $i$ with respect to each Gaussian component $j$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\gamma_{i,j} = \\frac{\\phi (\\mathbf{x}_i ; \\mathbf{\\mu}_j, \\mathbf{\\Sigma}_j) \\alpha_j}{\\sum_t \\phi (\\mathbf{x}_i ; \\mathbf{\\mu}_t, \\mathbf{\\Sigma}_t) \\alpha_t}\n",
        "\\end{equation}\n",
        "\n",
        "The sum in the denominator is a normalising factor that ensures the responsibilities for each sample sum to 1.\n",
        "\n",
        "In the **M-step**, the model parameters are updated from the data according to the responsibilities:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{\\mu}_j &= \\frac{\\sum_i \\gamma_{i,j} \\mathbf{x}_i}{\\sum_i \\gamma_{i,j}} \\\\\n",
        "\\mathbf{\\Sigma}_j &= \\frac{\\sum_i \\gamma_{i,j} (\\mathbf{x}_i - \\mathbf{\\mu}_j)(\\mathbf{x}_i - \\mathbf{\\mu}_j)^{\\mathsf{T}}}{\\sum_i \\gamma_{i,j}}  \\\\\n",
        "\\alpha_j &= \\frac{\\sum_i \\gamma_{i,j}}{n}\n",
        "\\end{align}\n",
        "\n",
        "Note that for these purposes the vectors should be considered as **column** vectors, so the product in the numerator of the second line above is an **outer product**, whose result is a $d \\times d$ matrix.\n",
        "\n",
        "The overall **log likelihood** of the model can be estimated as the sum of the log probabilities of all the observations given the Gaussian parameters, weighted by the class probabilities:\n",
        "\n",
        "\\begin{equation}\n",
        "l(\\mathbf{\\theta}) = \\sum_i^n \\log \\sum_j^k \\alpha_j \\phi(\\mathbf{x}_i ; \\mathbf{\\mu}_j, \\mathbf{\\Sigma}_j)\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "IxSZt7wnEGQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Calculate the log-likelihood of a GMM\n",
        "\n",
        "Implement the `gaussian_mix_loglik` function in the code cell below to calculate the likelihood of the model parameters given the data — that is, $l(\\mathbf{\\theta})$, as given above.\n",
        "\n",
        "Note that SciPy provides an object encapsulating the multivariate Gaussian density, [scipy.stats.multivariate_normal](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html), which we imported at the top of this notebook as `mvn`. The method `pdf` on this object can be used to evaluate the probability of one or more data samples."
      ],
      "metadata": {
        "id": "XpmiaCk1CWur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_mix_loglik ( X, means, covs, class_probs ):\n",
        "    \"\"\"\n",
        "    Estimate the log likelihood of the given mixture model.\n",
        "\n",
        "    # Arguments\n",
        "        X: a matrix of sample inputs, where\n",
        "            the samples are the rows and the\n",
        "            columns are features, ie size is:\n",
        "              num_samples x num_features\n",
        "        means: a list of vectors specifying mean of each gaussian\n",
        "            (all the same length == the number of features)\n",
        "        covs: a list of covariance matrices\n",
        "            (same length as means, with each matrix being\n",
        "            num features x num features, symmetric and\n",
        "            positive semidefinite)\n",
        "        class_probs: a vector of class probabilities,\n",
        "            (same length as means, all non-negative and\n",
        "            summing to 1)\n",
        "\n",
        "    # Returns\n",
        "        loglik: the (scalar) log likelihood of the model\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "b8rOxkB-CdjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Perform the E-step\n",
        "\n",
        "Implement `gaussian_mix_E_step` in the code cell below to estimate the **responsibilities**, $\\gamma_{i,j}$ — ie, the degree to which we attribute each data sample to each Gaussian component. Note that in this step we take all the distribution parameters as given.\n",
        "\n",
        "Again, you will probably want to use [`mvn.pdf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html)."
      ],
      "metadata": {
        "id": "glvcsU6MKYKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_mix_E_step ( X, means, covs, class_probs ):\n",
        "    \"\"\"\n",
        "    Given a candidate set of gaussian mix parameters,\n",
        "    estimate the responsiblilites of each component for\n",
        "    each data sample.\n",
        "\n",
        "    # Arguments\n",
        "        X: a matrix of sample inputs, where\n",
        "            the samples are the rows and the\n",
        "            columns are features, ie size is:\n",
        "              num_samples x num_features\n",
        "        means: a list of vectors specifying mean of each gaussian\n",
        "            (all the same length == the number of features)\n",
        "        covs: a list of covariance matrices\n",
        "            (same length as means, with each matrix being\n",
        "            num features x num features, symmetric and\n",
        "            positive semidefinite)\n",
        "        class_probs: a vector of class probabilities,\n",
        "            (same length as means, all non-negative and\n",
        "            summing to 1)\n",
        "\n",
        "    # Returns\n",
        "        resps: a matrix of weights attributing\n",
        "            samples to source gaussians, of size\n",
        "              num_samples x num_gaussians\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "ONQaA_DxKdzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Perform the M-step\n",
        "\n",
        "Implement `gaussian_mix_M_step` in the code cell below to update the GMM parameters. In this case, the *responsibilities* are taken as given."
      ],
      "metadata": {
        "id": "N3HuOp6tKeOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_mix_M_step ( X, resps ):\n",
        "    \"\"\"\n",
        "    Given a candidate set of responsibilities,\n",
        "    estimate new gaussian mixture model parameters.\n",
        "\n",
        "    # Arguments\n",
        "        X: a matrix of sample inputs, where\n",
        "            the samples are the rows and the\n",
        "            columns are features, ie size is:\n",
        "              num_samples x num_features\n",
        "        resps: a matrix of weights attributing\n",
        "            samples to source gaussians, of size\n",
        "              num_samples x num_gaussians\n",
        "\n",
        "    # Returns\n",
        "        means: a list of vectors specifying mean of each gaussian\n",
        "            (all the same length == the number of features)\n",
        "        covs: a list of covariance matrices\n",
        "            (same length as means, with each matrix being\n",
        "            num features x num features, symmetric and\n",
        "            positive semidefinite)\n",
        "        class_probs: a vector of class probabilities,\n",
        "            (same length as means, all non-negative and\n",
        "            summing to 1)\n",
        "    \"\"\"\n",
        "    # TODO: implement this\n",
        "    return None, None, None"
      ],
      "metadata": {
        "id": "KPXdi1GLKh3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Iteratively fit the model\n",
        "\n",
        "Implement `fit_gaussian_mix` in the code cell below to actually do the fitting.\n",
        "\n",
        "Use the functions you implemented in Tasks 2.1–2.3 to perform the EM steps and to evaluate the likelihood. You will need to choose an appropriate set of `num_gaussians` initial values for the means, covariances and class probabilities."
      ],
      "metadata": {
        "id": "pflDYjY7Kjrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_gaussian_mix ( X, num_gaussians, rng, max_iter=10,\n",
        "                       loglik_stop=1e-1 ):\n",
        "    \"\"\"\n",
        "    Fit a gaussian mixture model to some data.\n",
        "\n",
        "    # Arguments\n",
        "        X: a matrix of sample inputs, where\n",
        "            the samples are the rows and the\n",
        "            columns are features, ie size is:\n",
        "              num_samples x num_features\n",
        "        num_gaussians: the number of components\n",
        "            to fit\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "        max_iter: the maximum number of iterations\n",
        "            to perform\n",
        "        loglik_stop: stop iterating once the improvement\n",
        "            in log likelihood drops below this\n",
        "\n",
        "    # Returns\n",
        "        resps: a matrix of weights attributing\n",
        "            samples to source gaussians, of size\n",
        "              num_samples x num_gaussians\n",
        "        means: a list of num_gaussians vectors specifying means\n",
        "        covs: a list of num_gaussians covariance matrices\n",
        "        class_probs: a vector of num_gaussian class probabilities\n",
        "        logliks: a list of the model log likelihood values after\n",
        "            each fitting iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None, None, None, None, None"
      ],
      "metadata": {
        "id": "gJD_VeqLKoOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 2\n",
        "\n",
        "Run the cell below to actually fit a Gaussian Mixture Model to the data generated in Task 1, and plot the results. How does it do? Is the fit convincing? Does the dataset seem easy or hard? Does the likelihood improve consistently?\n",
        "\n",
        "By default we fit the same number of components here as we originally generated, but in reality we might not know how many there are. Try changing `FIT_GAUSSIANS` to fit more components, or fewer. Again, how does it do?"
      ],
      "metadata": {
        "id": "TgYKGJDIHajj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NB: this uses the data generated and plotted in Task 1\n",
        "\n",
        "# fitting params - you might want to play with these\n",
        "FIT_GAUSSIANS = NUM_GAUSSIANS\n",
        "MAX_ITER = 10\n",
        "\n",
        "fit_resps, fit_means, fit_covs, fit_class_probs, fit_logliks = fit_gaussian_mix ( X, num_gaussians=FIT_GAUSSIANS,\n",
        "                                                                                  rng=shared_rng, max_iter=MAX_ITER )\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "axs = fig.subplots(ncols=2)\n",
        "\n",
        "if fit_resps is None:\n",
        "    utils.plot_unimplemented(axs[0], title='Fitting History')\n",
        "    utils.plot_unimplemented(axs[1], title=f'Fit to {FIT_GAUSSIANS} Gaussians')\n",
        "else:\n",
        "    axs[0].plot(np.arange(len(fit_logliks))+1, fit_logliks)\n",
        "    axs[0].set_title('Fitting History')\n",
        "    axs[0].set_xlabel('Iteration')\n",
        "    axs[0].set_ylabel('Model Log Likelihood');\n",
        "\n",
        "    y_hat = np.argmax(fit_resps, axis=1)\n",
        "    for ii in range(FIT_GAUSSIANS):\n",
        "        idx = (y_hat == ii)\n",
        "        axs[1].scatter(X[idx,0], X[idx,1], color=LIGHT[ii])\n",
        "\n",
        "    # we'll again add contours for each (fitted) gaussian\n",
        "    left, right = axs[1].get_xlim()\n",
        "    bottom, top = axs[1].get_ylim()\n",
        "    xx = np.linspace(left, right, 100)\n",
        "    yy = np.linspace(bottom, top, 100)\n",
        "    grid = np.moveaxis(np.stack(np.meshgrid(xx, yy, indexing='xy')), 0, -1)\n",
        "    extent = (left, right, bottom, top)\n",
        "\n",
        "    # draw the contours and centre points in the appropriate colours\n",
        "    for ii in range(FIT_GAUSSIANS):\n",
        "        pdf = mvn.pdf(grid, mean=fit_means[ii], cov=fit_covs[ii])\n",
        "        axs[1].contour(xx, yy, pdf, origin='lower', extent=extent, alpha=0.5, colors=[DARK[ii]])\n",
        "        axs[1].scatter(fit_means[ii][0], fit_means[ii][1], s=100, color=DARK[ii], marker='x')\n",
        "\n",
        "    axs[1].set_title(f'Fit to {FIT_GAUSSIANS} Gaussians')\n",
        "    axs[1].set_xlabel('$x_1$')\n",
        "    axs[1].set_ylabel('$x_2$');\n",
        "\n",
        "fig.tight_layout(pad=1)\n"
      ],
      "metadata": {
        "id": "k8SxT4xfG_3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Generate data from a Hidden Markov Model\n",
        "\n",
        "In a hidden Markov model, the visible output is conditioned on some unobserved **latent state** that varies over time, with the state at any discrete time step $t$, $z_t$, depending only on the state at the previous time step, $z_{t-1}$.\n",
        "\n",
        "For these exercises we will consider only scalar observations, $x_t$, whose values are normally distributed with mean and variance according to the latent state:\n",
        "\\begin{equation}\n",
        "(x_t | z_t = j) \\sim N(\\mu_j, \\sigma_j^2 )\n",
        "\\end{equation}\n",
        "So the parameters $\\mu_1, \\mu_2, ..., \\mu_k$ and $\\sigma_1, \\sigma_2, ... , \\sigma_k$ define the **emission probabilities**.\n",
        "\n",
        "The transition probabilities are defined by a $k \\times k$  **transition matrix** in which the element at row $i$ and column $j$ specifies the probability of going from state $i$ to state $j$, ie:\n",
        "\n",
        "\\begin{equation}\n",
        "a_{i,j} = P(z_{t+1} = j | z_t = i)\n",
        "\\end{equation}\n",
        "\n",
        "The distribution of starting states of the model is determined by an **initial probability** vector $\\mathbf{\\pi} = [\\pi_1, \\pi_2, ... , \\pi_k]$, where\n",
        "\n",
        "\\begin{equation}\n",
        "\\pi_j = P(z_1 = j)\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "FMWP8eI3Ba46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Generate the data\n",
        "\n",
        "Implement `generate_hmm_sequence` in the code cell below to generate data from a fully parameterised HMM.\n",
        "\n",
        "The [choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) method of [numpy.random.Generator](https://numpy.org/doc/stable/reference/random/generator.html) is useful for drawing single values from a categorical distribution. (In contrast, the [multinomial](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multinomial.html) method models the outcomes of multiple trials, returning *counts* rather than single values.)\n",
        "\n",
        "As in Task 1.1, you are asked to return the true hidden state sequence as well as the observations for plotting purposes. Obviously these would not be available for real data."
      ],
      "metadata": {
        "id": "vE-Bf4Zai18i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hmm_sequence ( num_samples,\n",
        "                            initial_probs, transitions,\n",
        "                            emission_means, emission_sds,\n",
        "                            rng ):\n",
        "    \"\"\"\n",
        "    Generate a sequence of observations from a hidden Markov\n",
        "    model with the given parameters. Emissions are univariate\n",
        "    Gaussians.\n",
        "\n",
        "    # Arguments\n",
        "        num_samples: number of samples (ie timesteps) to generate\n",
        "        initial_probs: vector of probabilities of being in each hidden\n",
        "            state at time step 1; must sum to 1\n",
        "        transitions: matrix of transition probabilities, from state\n",
        "            indexed by row to state indexed by column; rows must sum to 1\n",
        "        emission_means: mean of observations for each hidden state\n",
        "        emission_sds: standard deviation of observations for each hidden state\n",
        "        rng: an instance of numpy.random.Generator\n",
        "            from which to draw random numbers\n",
        "\n",
        "    # Returns\n",
        "        x: a vector of observations for each time step\n",
        "        z: a vector of hidden state (indices) for each time step\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "MQrURpx5i1ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 3\n",
        "\n",
        "Run the cell below to generate a sequence of samples from your HMM function, plotting both the observations and the underlying states."
      ],
      "metadata": {
        "id": "7cOF1Dyfig7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HMM parameters are randomised around some configuration details\n",
        "\n",
        "# how much data to generate\n",
        "NUM_TIMESTEPS = 500\n",
        "\n",
        "# how many hidden states are there?\n",
        "NUM_HIDDEN = 3\n",
        "\n",
        "# how strong is our baseline tendency to remain in the same state\n",
        "DWELL = 10\n",
        "\n",
        "# how noisy is our emission output\n",
        "NOISE = 0.2\n",
        "\n",
        "# how uniform are our start probabilities\n",
        "START_SIMILARITY = 0.1\n",
        "\n",
        "initial_probs = shared_rng.random(NUM_HIDDEN) + START_SIMILARITY\n",
        "initial_probs /= np.sum(initial_probs)\n",
        "\n",
        "# randomise transitions with a dwell bias on the main diagonal\n",
        "transitions = shared_rng.random((NUM_HIDDEN, NUM_HIDDEN)) + np.eye(NUM_HIDDEN) * DWELL\n",
        "\n",
        "# scale each row to sum to 1\n",
        "for ii in range(NUM_HIDDEN):\n",
        "    transitions[ii,:] /= np.sum(transitions[ii,:])\n",
        "\n",
        "# means are just evenly spaced for simplicity\n",
        "# (mainly this makes the plotting nice and easy)\n",
        "emission_means = np.arange(NUM_HIDDEN)\n",
        "\n",
        "# variances are randomised\n",
        "emission_sds = shared_rng.random(NUM_HIDDEN) * NOISE\n",
        "\n",
        "x, z = generate_hmm_sequence( NUM_TIMESTEPS, initial_probs, transitions,\n",
        "                              emission_means, emission_sds, shared_rng )\n",
        "\n",
        "fig = plt.figure(figsize=(6,6))\n",
        "ax = fig.subplots()\n",
        "\n",
        "ax.plot(x, color=DARK[1], label='Observed')\n",
        "ax.plot(z, color=DARK[0], label='Hidden')\n",
        "ax.set_title(f'Hidden Markov Model')\n",
        "ax.set_xlabel('Time')\n",
        "ax.set_ylabel('Value')\n",
        "ax.legend(loc='upper right')\n",
        "\n",
        "fig.tight_layout(pad=1)"
      ],
      "metadata": {
        "id": "o-zoeOPlLG15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Decode the underlying state sequence using the Viterbi algorithm\n",
        "\n",
        "The Viterbi algorithm iterates forwards through the sequence, at each timestep $t$ estimating the highest probability of being in each state, given these factors:\n",
        "\n",
        "* the present observation $x_t$\n",
        "* the probability of transition to $z_t$ from each previous state $z_{t-1} = j$\n",
        "* the previously estimated probability of being in that previous state, $\\alpha_{t-1}(j)$\n",
        "\n",
        "The recurrence relation looks like this:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha_t(j) = \\max_i \\; \\alpha_{t-1}(i) \\, a_{i,j} \\, P(x_t ; \\mu_j, \\sigma_j)\n",
        "\\end{equation}\n",
        "\n",
        "For each state at each step it also records which previous state produced that highest probability as a **backpointer**, $\\zeta_t(j)$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\zeta_t(j) = \\text{argmax}_i \\; \\alpha_{t-1}(i) \\, a_{i,j} \\, P(x_t ; \\mu_j, \\sigma_j)\n",
        "\\end{equation}\n",
        "\n",
        "At the end of the sequence, timestep $T$, the most likely final state is identified:\n",
        "\n",
        "\\begin{equation}\n",
        "z_T = \\text{argmax}_j \\alpha_T(j)\n",
        "\\end{equation}\n",
        "\n",
        "and the $\\zeta_t$ are traced backwards to reconstruct the complete most likely sequence:\n",
        "\\begin{equation}\n",
        "z_{t-1} = \\zeta_t(z_t)\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "xiLN1eJ9LBtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Find the most likely hidden state sequence\n",
        "\n",
        "Implement the `viterbi` function in the cell below to infer a hidden state sequence from observations using the algorithm described above. Note that this algorithm only infers the states — it assumes the HMM parameters are already known.\n",
        "\n",
        "Although the calculations as given above are not enormously complicated, this can be a bit fiddly to implement. You will need to build a $k \\times T$ \"trellis\" matrix for both the $\\alpha$ and $\\zeta$ values. As with long chains of gradients in backpropagation, the probability products here can get vanishingly small and make things numerically unstable, so it's probably a good idea to normalise at each timestep.\n",
        "\n",
        "The SciPy method [scipy.stats.norm.pdf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html) evaluates the univariate Gaussian density. As with `mvn`, we imported the class back at the start, so you can invoke it as just `norm.pdf(...)`."
      ],
      "metadata": {
        "id": "H-OqhwSaLLnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi ( x, initial_probs, transitions, emission_means, emission_sds ):\n",
        "    \"\"\"\n",
        "    Infer the most likely sequence of hidden states based on\n",
        "    observations and HMM parameters.\n",
        "\n",
        "    # Arguments\n",
        "        x: a vector of observations for each time step\n",
        "        initial_probs: vector of probabilities of being in each hidden\n",
        "            state at time step 1; must sum to 1\n",
        "        transitions: matrix of transition probabilities, from state\n",
        "            indexed by row to state indexed by column; rows must sum to 1\n",
        "        emission_means: mean of observations for each hidden state\n",
        "        emission_sds: standard deviation of observations for each hidden state\n",
        "\n",
        "    # Returns\n",
        "        z: a vector of predicted hidden state (indices) for each time step\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implement this\n",
        "    return None"
      ],
      "metadata": {
        "id": "uPXP0GZlLQVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## → Run Task 4\n",
        "\n",
        "Run the cell below to infer the hidden states from the observations using your Viterbi implementation above.\n",
        "\n",
        "As a point of comparison, the test code also employs the  [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/) package to infer the hidden states — but without any knowledge of the underlying HMM parameters (other than the number of states). So it must first infer those parameters before decoding the sequence. This is a *significantly* harder task than Viterbi alone, so the resulting fit will almost certainly be much worse than yours.\n",
        "\n",
        "As ever, play around with the parameters governing both the data generation and the fitting to see what effects they have."
      ],
      "metadata": {
        "id": "BEg5cmlpLRmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fit_z = viterbi ( x, initial_probs, transitions, emission_means, emission_sds )\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "axs = fig.subplots(ncols=2)\n",
        "\n",
        "if fit_z is None:\n",
        "    utils.plot_unimplemented(axs[0], title=f'Viterbi Decoding (True Model)')\n",
        "else:\n",
        "    axs[0].plot(z, color=DARK[1], label='True')\n",
        "    axs[0].plot(fit_z, color=DARK[0], label='Decoded')\n",
        "    axs[0].set_title(f'Viterbi Decoding (True Model)')\n",
        "    axs[0].set_xlabel('Time')\n",
        "    axs[0].set_ylabel('State')\n",
        "    axs[0].legend(loc='upper right')\n",
        "\n",
        "# infer model params using hmmlearn\n",
        "hmg = hmm.GaussianHMM(NUM_HIDDEN)\n",
        "hmg.fit(x.reshape(-1,1))\n",
        "hfit_z = hmg.predict(x.reshape(-1,1))\n",
        "\n",
        "axs[1].plot(z, color=DARK[1], label='True')\n",
        "axs[1].plot(hfit_z.ravel(), color=DARK[0], label='Decoded')\n",
        "axs[1].set_title(f'Viterbi Decoding (Fitted Model)')\n",
        "axs[1].set_xlabel('Time')\n",
        "axs[1].set_ylabel('State')\n",
        "axs[1].legend(loc='upper right')\n",
        "\n",
        "fig.tight_layout(pad=1)"
      ],
      "metadata": {
        "id": "_IbSTCvVLWmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further exploration\n",
        "\n",
        "If you're feeling adventurous, you could try implementing the [Baum-Welch algorithm](https://en.wikipedia.org/wiki/Baum–Welch_algorithm) to fit the model parameters. This is the actual expectation-maximisation procedure that `hmmlearn` is doing behind the scenes. Needless to say, it is significantly more fiddly than Viterbi.\n"
      ],
      "metadata": {
        "id": "ENAVpErdCTaX"
      }
    }
  ]
}